{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "train_origin_data = 'data.orig'\n",
    "annotated_data = 'Train_output'\n",
    "entity_type_list = [\"d\", \"s\", \"c\", \"i\", \"a\", \"b\", \"t\", \"p\"]\n",
    "all_data = [\"raw_data.orig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_line(line_num, origin_line, line, entity_type_list):\n",
    "    words = []\n",
    "    tags = []\n",
    "    for part in line.split(\" \"):\n",
    "        part = part.strip(\"\\n\")\n",
    "        if len(part) >= 2 and part[-2] == '\\\\' and part[-1] in entity_type_list:\n",
    "            words.extend(list(part[ :-2]))\n",
    "            part_tag = [\"B-\" + part[-1].upper()] + [\"I-\" + part[-1].upper()] * (len(part) - 3)\n",
    "            tags.extend(part_tag)\n",
    "        else:\n",
    "            words.extend(list(part))\n",
    "            part_tag = [\"O\"] * len(part)\n",
    "            tags.extend(part_tag)\n",
    "    # print(len(words))\n",
    "    # print(len(tags))\n",
    "    assert len(words) == len(tags)\n",
    "    if(list(origin_line.strip()) != words):\n",
    "        print(\"At line {} origin and annotated line don't match! \".format(line_num))\n",
    "    assert words == list(origin_line.strip())\n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpora(data_path, train_origin_data,train_annotated_data,entity_type_list):\n",
    "    data_origin = os.path.join(data_path, train_origin_data)\n",
    "    annotation = os.path.join(data_path, train_annotated_data)\n",
    "    with open(data_origin) as f1:\n",
    "        origin_lines = f1.readlines()\n",
    "    with open(annotation) as f2:\n",
    "        annotated_lines = f2.readlines()\n",
    "    assert len(origin_lines) == len(annotated_lines)\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    train_data = []\n",
    "    for i in range(len(origin_lines)):\n",
    "        # cache1 = origin_lines[i]\n",
    "        # cache2 = annotated_lines[i]\n",
    "        # print(f'length origin lines:{len(cache1)}')\n",
    "        # print(f'length annotated lines:{len(cache2)}')\n",
    "        # break\n",
    "        words, tags = resolve_line(i+1, origin_lines[i], annotated_lines[i], entity_type_list)\n",
    "        train_data.append((words, tags))\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(re_true,_predict):\n",
    "    test_acc = 0.0\n",
    "    cnt = 0.0\n",
    "    for i in range(len(re_true)):\n",
    "            if re_true[i] == _predict[i]:\n",
    "                    cnt += 1\n",
    "            else:\n",
    "                    pass\n",
    "    test_acc = cnt/len(re_true)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc:0.8797653958944281\n"
     ]
    }
   ],
   "source": [
    "pre_data = read_corpora(data_path, train_origin_data,annotated_data, entity_type_list)\n",
    "# print('Data is preprocessed!')\n",
    "_predict = []\n",
    "\n",
    "for i in range(len(pre_data)):\n",
    "        _predict.append(pre_data[i][1])\n",
    "\n",
    "\n",
    "\n",
    "groud_truth = 'Ground_truth'\n",
    "groud_truth = os.path.join(data_path, groud_truth)\n",
    "with open(groud_truth) as f:\n",
    "        _true_labels = f.readlines()\n",
    "\n",
    "rows = len(_true_labels)\n",
    "re_true = []\n",
    "for i in range(rows):\n",
    "        cache = _true_labels[i].strip().split(' ')\n",
    "        re_true.append(cache)\n",
    "\n",
    "\n",
    "# print('Total true labels:',len(re_true))\n",
    "# print('length of 1st true labels:',len(re_true[0]))\n",
    "# print('Total pre labels:',len(pre_data))\n",
    "# print('Total pre labels:',len(_predict))\n",
    "# print('length of 1st pre labels:',len(_predict[0]))\n",
    "\n",
    "# print('1st pre labels:',_predict[0])\n",
    "# print('1st true labels:',re_true[0])\n",
    "\n",
    "test_acc = calculate_accuracy(re_true[0],_predict[0])\n",
    "print(f'test_acc:{test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.7593457558631598\n",
      "precision:0.6743578777264214\n",
      "recall:0.7367999857253221\n",
      "f1:0.6812175802171195\n",
      "mat:[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   7   0   0   1   0   0   1   0   0   0   0   0]\n",
      " [  0   0   0   3   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  10   0   0   0   0   0   2   0   0]\n",
      " [  0   0   0   0   0   0   4   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  13   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  40   0   0   3   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  19   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  36   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  15   0]\n",
      " [  0   1   2   2   0   2   0   0   7  10   0   7   0 152]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer  \n",
    "\n",
    "acc = 0.0\n",
    "precision = 0.0\n",
    "recall = 0.0\n",
    "f1 = 0.0\n",
    "for i in range(rows):\n",
    "\n",
    "\n",
    "    acc += calculate_accuracy(re_true[i],_predict[i])/rows\n",
    "    precision += precision_score(re_true[i],_predict[i],average='macro')/rows\n",
    "    recall += recall_score(re_true[i],_predict[i],average='macro')/rows\n",
    "    f1 += f1_score(re_true[i],_predict[i],average='macro')/rows\n",
    "\n",
    "\n",
    "mat = confusion_matrix(np.array(re_true[0]),np.array(_predict[0]))\n",
    "\n",
    "\n",
    "print(f'acc:{acc}')\n",
    "print(f'precision:{precision}')\n",
    "print(f'recall:{recall}')\n",
    "print(f'f1:{f1}')\n",
    "\n",
    "print(f'mat:{mat}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ada863ba5d8596ec241140439045809b47de0161a1b82a0e7ba4b5571a9dbf"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
