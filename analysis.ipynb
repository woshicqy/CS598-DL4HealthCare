{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "train_origin_data = 'data.orig'\n",
    "train_annotated_data = 'Train_output'\n",
    "entity_type_list = [\"d\", \"s\", \"c\", \"i\", \"a\", \"b\", \"t\", \"p\"]\n",
    "all_data = [\"raw_data.orig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_line(line_num, origin_line, line, entity_type_list):\n",
    "    words = []\n",
    "    tags = []\n",
    "    for part in line.split(\" \"):\n",
    "        part = part.strip(\"\\n\")\n",
    "        if len(part) >= 2 and part[-2] == '\\\\' and part[-1] in entity_type_list:\n",
    "            words.extend(list(part[ :-2]))\n",
    "            part_tag = [\"B-\" + part[-1].upper()] + [\"I-\" + part[-1].upper()] * (len(part) - 3)\n",
    "            tags.extend(part_tag)\n",
    "        else:\n",
    "            words.extend(list(part))\n",
    "            part_tag = [\"O\"] * len(part)\n",
    "            tags.extend(part_tag)\n",
    "    # print(len(words))\n",
    "    # print(len(tags))\n",
    "    assert len(words) == len(tags)\n",
    "    if(list(origin_line.strip()) != words):\n",
    "        print(\"At line {} origin and annotated line don't match! \".format(line_num))\n",
    "    assert words == list(origin_line.strip())\n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpora(data_path, train_origin_data,train_annotated_data,entity_type_list, save_path):\n",
    "    data_origin = os.path.join(data_path, train_origin_data)\n",
    "    annotation = os.path.join(data_path, train_annotated_data)\n",
    "    with open(data_origin) as f1:\n",
    "        origin_lines = f1.readlines()\n",
    "    with open(annotation) as f2:\n",
    "        annotated_lines = f2.readlines()\n",
    "    assert len(origin_lines) == len(annotated_lines)\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    train_data = []\n",
    "    for i in range(len(origin_lines)):\n",
    "        # cache1 = origin_lines[i]\n",
    "        # cache2 = annotated_lines[i]\n",
    "        # print(f'length origin lines:{len(cache1)}')\n",
    "        # print(f'length annotated lines:{len(cache2)}')\n",
    "        # break\n",
    "        words, tags = resolve_line(i+1, origin_lines[i], annotated_lines[i], entity_type_list)\n",
    "        train_data.append((words, tags))\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    f.close()\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total true labels 197\n",
      "Total pre labels 197\n",
      "Total pre labels 197\n"
     ]
    }
   ],
   "source": [
    "pre_data = read_corpora(data_path, train_origin_data,train_annotated_data, entity_type_list, os.path.join(data_path, \"train_data.pkl\"))\n",
    "# print('Data is preprocessed!')\n",
    "predict = []\n",
    "\n",
    "for i in range(len(pre_data)):\n",
    "        predict.append(pre_data[i][1])\n",
    "\n",
    "\n",
    "\n",
    "groud_truth = 'Ground_truth'\n",
    "groud_truth = os.path.join(data_path, groud_truth)\n",
    "with open(groud_truth) as f:\n",
    "        true_label = f.readlines()\n",
    "print('Total true labels',len(true_label))\n",
    "print('Total pre labels',len(pre_data))\n",
    "print('Total pre labels',len(predict))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.9086294416243654\n",
      "precision:0.952755905511811\n",
      "recall:1.0\n",
      "f1:0.9758064516129031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer  \n",
    "\n",
    "true_label = MultiLabelBinarizer().fit_transform(true_label)\n",
    "predict = MultiLabelBinarizer().fit_transform(predict)\n",
    "\n",
    "acc = accuracy_score(true_label,predict)\n",
    "precision = precision_score(true_label,predict,average='micro')\n",
    "recall = recall_score(true_label,predict,average='micro')\n",
    "f1 = f1_score(true_label,predict,average='micro')\n",
    "\n",
    "\n",
    "print(f'acc:{acc}')\n",
    "print(f'precision:{precision}')\n",
    "print(f'recall:{recall}')\n",
    "print(f'f1:{f1}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ada863ba5d8596ec241140439045809b47de0161a1b82a0e7ba4b5571a9dbf"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
